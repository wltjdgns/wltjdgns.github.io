# 논문 리뷰: HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images (CVPR 2023)  
**Authors:** Animesh Karnewar* (UCL), Andrea Vedaldi (Meta AI), David Novotny* (Meta AI), Niloy J. Mitra (UCL)  
<small>*Equal contribution (Karnewar & Novotny)</small>  

## Abstract Summary  
**HoloDiffusion** is introduced as the first unconditional *3D-aware generative diffusion model* that can be trained using only **2D images with known poses** (no 3D ground truth) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=generative%20modeling%20of%202D%20images,the%20first%20challenge%20by%20introducing)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=In%20this%20paper%2C%20we%20contribute,de%02fined%20in%203D%20space%2C%20the)). The abstract explains two major challenges in extending diffusion models to 3D: (1) the scarcity of large-scale 3D training data compared to 2D, and (2) the cubic growth in memory and computation when moving from 2D to 3D grids ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=due%20to%20the%20possibility%20of,the%20first%20challenge%20by%20introducing)). To tackle these, the authors propose a new **end-to-end diffusion training setup with only posed 2D image supervision** (addressing challenge 1), and a novel **image formation model that decouples model memory from spatial memory** (addressing challenge 2) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=conceptually%20trivial%20to%20extend%20the,has%20not%20been%20used%20to)). They evaluate HoloDiffusion on real-world data (the CO3D/CO3Dv2 dataset) and find that their 3D diffusion models are **scalable, train robustly**, and produce competitive sample quality and fidelity compared to existing 3D generative approaches ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=data%2C%20using%20the%20CO3D%20dataset,ap%02proaches%20for%203D%20generative%20modeling)). In summary (Abstract), HoloDiffusion demonstrates that 3D-consistent diffusion models can be learned from abundant 2D images with stable objectives, without requiring large 3D model collections ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=complexity%20makes%20this%20infeasible,terms%20of%20sample%20quality%20and)).  

## 1. Introduction  
Diffusion models have rapidly become state-of-the-art generative models for images, surpassing GANs and VAEs in applications like colorization, editing, and synthesis ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=1,9%5D%20than%20previous%20alternatives)). They succeed in part because they optimize likelihood directly and can be trained on **very large image datasets** with a stable objective ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=generative%20models%20for%20images%2C%20replacing,9%5D%20than%20previous%20alternatives)). A natural next step is to bring diffusion models to **3D data**, which would enable direct manipulation of generated content, perfect multi-view consistency, and placement of objects in 3D scenes ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=A%20natural%20next%20step%20is,Researchers%20have)). However, two key obstacles hinder *learning 3D diffusion models*: (1) the lack of sufficient 3D training data, and (2) the choice of 3D representation, as naive 3D grids cause infeasible memory growth ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=with%20a%20stable%20learning%20objective,with%20only%20posed%202D%20images)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=conceptually%20trivial%20to%20extend%20the,has%20not%20been%20used%20to)). Prior works have begun exploring 3D diffusion, e.g. for point clouds or radiance fields ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=fields%20of%203D%20objects%20,49)), but **no diffusion-based 3D generator trained using only 2D image supervision existed** before this work ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=fields%20of%203D%20objects%20,a%20new%203D%20model%20that)). 

**HoloDiffusion** is presented to fill this gap as *the first 3D diffusion model trained on only real posed 2D images* ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=In%20this%20paper%2C%20we%20contribute,de%02fined%20in%203D%20space%2C%20the)). Here “posed” means each training image comes with its camera parameters (e.g. from structure-from-motion) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=unconditional%203D%20diffusion%20model%20that,rendered%20images%20are%20consistent%20across)). The introduction highlights **two main technical contributions** of HoloDiffusion (Introduction):  

- **(i) Hybrid explicit–implicit 3D representation:** They design a 3D model using a **voxelized feature grid** (an implicit feature volume) rather than an explicit density field ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=We%20make%20two%20main%20technical,the%20grid%20can%20be%20decoupled)). This **feature grid** can be rendered from any viewpoint, and because features are defined in 3D space, the rendered images are consistent across different views ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=feature%20grid,%28ii%29%20We%20design)). Compared to an explicit voxel density grid, the learned feature representation allows using a lower resolution grid (since features can encode appearance details), which makes estimating a probability distribution easier (fewer variables) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=rendered%20images%20are%20consistent%20across,Specifically)). Moreover, the grid resolution is decoupled from the output image resolution, so one can render high-res images from a moderate-sized 3D grid ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=feature%20representation%20allows%20for%20a,on%20the%20input%20posed%20images)).

- **(ii) Novel diffusion training method with 2D supervision:** They introduce a diffusion modeling approach that learns a distribution over these 3D feature grids **using only 2D images for supervision** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=from%20the%20resolution%20of%20the,error%20between%20the%20rendered%20images)). The procedure is: first, given input images with poses, generate an intermediate **3D-aware feature grid** (using only the images); then add noise to this intermediate 3D representation and train a *3D U-Net denoiser* to remove the noise ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=such%203D%20feature%20grids%20while,The%20key%20advantage)). The denoising loss is applied **as a photometric error between rendered images of the denoised 3D grid and the ground-truth images** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Then%2C%20following%20the%20standard%20diffusion,The%20key%20advantage)). By enforcing the rendered outputs to match real images, the model learns to produce correct 3D features. This strategy enables training a 3D diffusion model from widely-available 2D images, sidestepping the need for huge datasets of 3D shapes ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=rendered%20images%20and%20the%20Ground,of%203D%20models%20for%20training)). 

Thanks to these ideas, HoloDiffusion can learn 3D generative models without any explicit 3D supervision. The introduction notes that they train and evaluate on the **Co3Dv2 dataset [46]** (a large dataset of multi-view videos of objects), and **HoloDiffusion outperforms existing alternatives** qualitatively and quantitatively on this data ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=We%20train%20and%20evaluate%20our,alter%02natives%20both%20qualitatively%20and%20quantitatively)). In short, the introduction positions HoloDiffusion as a **scalable and robust 3D diffusion framework** that leverages only posed image collections for training, overcoming prior limitations in data and memory requirements. Figure 1 illustrates example 3D-consistent images generated by HoloDiffusion for various object categories from Co3Dv2 (e.g. apples, hydrants, toys) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%201,46)), demonstrating multi-view consistency in its outputs.

## 2. Related Work  
**2.1. Image-Conditioned 3D Reconstruction:** This section reviews neural rendering and few-view 3D reconstruction methods that inform HoloDiffusion’s design. *Neural rendering* refers to using neural networks to approximate the image formation process (light transport) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Neural%20and%20differentiable%20rendering,neural%20rendering%20have%20recently%20been)). Early neural renderers in 2D included pix2pix and deferred rendering approaches that map coarse neural outputs to realistic RGB images ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=neural%20networks%20to%20approximate%20the,parameters%20of%20the%203D%20scene)). In 3D, the seminal Neural Radiance Fields (NeRF) approach uses an MLP to represent scene radiance and occupancy, rendering images via volumetric ray-marching ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=The%203D%20versions%20of%20neural,While%20the%20latter%20uses)). NeRF requires many images of a scene to recover its 3D shape and appearance (an inverse rendering problem) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=,scene%2C%20different%20representations%20were%20explored)). NeRF’s success spurred many variants exploring different scene representations (e.g. explicit voxel grids, point clouds, meshes) for differentiable rendering ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=and%20appearance,ambiguous%2C%20re%02cent%20methods%20aid%20the)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=MLPs%20to%20represent%20the%20occupancy,Works)).  

For *few-view 3D reconstruction*, where only a small number of views are available, recent methods incorporate learned 3D priors from object categories. For example, CMR, C3DM, UMR predict mesh parameters by training on many images of a category ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Since%203D%20reconstruction%20from%20few,videos%20of%20the%20deformable%20objects)). DOVE predicts meshes using video constraints ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=like%20CMR%20,videos%20of%20the%20deformable%20objects)). Other works fit NeRF given few views by sampling image features along rays (with transformers to aggregate features) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Others%20,with%20a%20signed%20distance%20function)). ViewFormer even learns new-view synthesis without an explicit renderer, and BANMo reconstructs deformable objects with an implicit surface model ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Others%20,with%20a%20signed%20distance%20function)). These approaches show ways to leverage category-level data to infer 3D structure from limited views, a scenario relevant to HoloDiffusion’s use of videos. Notably, they often **do not require explicit 3D supervision**, instead learning from images and known camera poses ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=The%20aforementioned%20approaches%20are%20trained,synthetic%20datasets)). HoloDiffusion similarly avoids needing ground-truth 3D shapes by using posed image sequences to supervise its generative model. However, unlike pure reconstruction pipelines, HoloDiffusion aims to *generate new instances*, not just fit existing ones.

**2.2. 3D Generative Models:** Earlier 3D generative models largely used adversarial training (GANs) as supervision ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3D%20Generative%20advesarial%20networks,from%20PlatonicGAN%20by%20focusing%20only)). *PlatonicGAN* learned to generate 3D shape (voxel grid) by rendering it from random views and adversarially matching real images ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=adversarial%20learning%20,To%20deal%20with%20the%20large)). *PrGAN* focused on untextured shapes ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=discriminator%20cannot%20distinguish%20between%20the,adjusts%20Platon%02icGAN%20by%20rendering%20a)). The memory issue of voxel grids was tackled by *HoloGAN*, which rendered a low-res feature map from a coarse voxel then used a 2D decoder ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=PrGAN%20,50%5D%20also)). HoloGAN achieved 2D photo-realism but not consistent geometry across views ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=memory%20footprint%20of%20voxels%2C%20HoloGAN,50%5D%20also)). With the advent of neural radiance fields, models like GRAF and pi-GAN adopted NeRFs as the scene representation while training in a GAN framework ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Inspired%20by%20the%20success%20of,52%5D%20architecture)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=triplane,64)). *GRAF* and its improved version *pi-GAN* represent each generated scene with a neural radiance field and learn from unposed image collections (assuming a distribution of camera viewpoints) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Inspired%20by%20the%20success%20of,52%5D%20architecture)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=isolated%20instances%20of%20object%20categories%2C,similar%20scene)). These produce impressive images but can suffer from multi-view inconsistencies or require carefully controlled pose distributions ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=supervision%20in%20form%20of%20the,estimation%20problem%20by%20leveraging%20a)). Other extensions include *GIRAFFE* and *BlockGAN* which compose multiple objects or background in a scene ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=poses%20that%20provide%20strong%20constraints,similar%20scene)), and text-conditioned 3D generation methods ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=isolated%20instances%20of%20object%20categories%2C,similar%20scene)). 

**3D diffusion models** are a very recent development (as of this paper). Luo *et al.* (2021) trained a diffusion model for point clouds with full 3D supervision ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3D%20diffusion%20models,view%20synthesis%20function%20which)). In a concurrent effort, Watson *et al.* (2022) learned a new-view synthesis model conditioned on a single input image using diffusion ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=learning%20have%20been%20explored%20only,may%20lead%20to%20geometrical%20inconsistencies)). However, Watson *et al.* did not employ an explicit 3D image formation model, which *may lead to geometric inconsistencies between views* ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=clouds,may%20lead%20to%20geometrical%20inconsistencies)). In contrast, HoloDiffusion uses an explicit volumetric rendering process (like NeRF) as part of its model, ensuring generated images from different viewpoints remain self-consistent. Thus, HoloDiffusion builds on the idea of diffusion for generative modeling but introduces a pipeline to handle 3D **with only image supervision**, distinguishing it from prior 3D GANs and early diffusion attempts.

## 3. HOLODIFFUSION  
Section 3 details the HoloDiffusion approach. First, the necessary background on diffusion models is given (Sec. 3.1), then the method is introduced step by step: Sec. 3.2 defines the learning setup with 2D videos, Sec. 3.3 presents the **Bootstrapped Latent Diffusion Model** (the core technique), and Sec. 3.4 covers implementation details ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3,4)).

### 3.1. Diffusion Models (Background)  
This section recaps the fundamentals of *Denoising Diffusion Probabilistic Models (DDPM)* ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=The%20Denoising%20Diffusion%20Probabilistic%20Model,N%7D%280%2C%20I%29.%20%282)) as used in HoloDiffusion. The task of generative modeling is defined: given i.i.d. samples $x_i$ from some data distribution $p(x)$, we seek a parametric model $p_\theta(x)$ that approximates $p(x)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Given%20N%20i.i.d.%20samples%20,data%20samples%20into%20pure%20noise)). Diffusion models achieve this by defining a **forward noising process** $q$ that gradually converts data $x_0$ into pure noise $x_T$ over $T$ time-steps ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=i%3D1%20from%20,samples%20into%20pure%20noise%2C%2018425)). In DDPM (as per Ho *et al.* [20]), the forward *Markov chain* is a Gaussian transition: 

- **Forward process:** at each step $t$, noise is added according to:  
  ***(Equation 1)*** $q(x_t \mid x_{t-1}) = \mathcal{N}\!\Big(x_t; \sqrt{\alpha_t}\,x_{t-1},\, (1-\alpha_t)\mathbf{I}\Big)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%28DDPM%29%20,N%7D%280%2C%20I%29.%20%282)).  
  This means $x_{t}$ is a noisy version of $x_{t-1}$, with a retained signal coefficient $\sqrt{\alpha_t}$ and added Gaussian noise of variance $(1-\alpha_t)$ (where $0<\alpha_t<1$). Typically, $\{\alpha_t\}$ is a **noise schedule** defined by $\alpha_t = 1 - \beta_t$ with $\beta_t$ increasing over time (e.g. linearly) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=where%2C%20the%20D%CE%B8%20is%20the,with%20T%20%3D%201000%20steps)). Because of this formulation, as $t$ increases, more noise is injected, and by the final step $T$ the sample is nearly an isotropic Gaussian ($x_T \sim \mathcal{N}(0,I)$) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=i,alpha%20_t%29I%29.%20%281)).

- **Reparameterization:** Equation (1) allows sampling $x_t$ directly if $x_{t-1}$ is known. By drawing $\epsilon \sim \mathcal{N}(0,I)$, one can express:  
  ***(Equation 2)*** $x_t = \sqrt{\alpha_t}\,x_{t-1} + \sqrt{\,1-\alpha_t\,}\,\epsilon$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Samples%20can%20be%20easily%20drawn,N%7D%280%2C%20I%29.%20%282)).  
  This is the “reparameterization trick” form, simply adding scaled noise $\epsilon$ to $x_{t-1}$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Samples%20can%20be%20easily%20drawn,N%7D%280%2C%20I%29.%20%282)). It’s useful for simulation and for deriving closed-form relations. For instance, by iterating this, one can sample $x_t$ at an arbitrary time $t$ directly from the original data $x_0$ in closed form: $x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon$, where $\bar\alpha_t = \prod_{i=1}^{t} \alpha_i$ (the cumulative product) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20target%20distribution%20p%28x%29,20%2C%2032)).

- **Reverse (generative) process:** The diffusion model trains a **denoising model** to invert this noising process. That is, we define a learned Gaussian $p_\theta(x_{t-1}|x_t)$ that should predict the distribution of $x_{t-1}$ given a noisy $x_t$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=i,alpha%20_t%29I%29.%20%281)). In DDPM, this reverse step is modeled as:  
  ***(Equation 3)*** $p_{\theta}(x_{t-1}\mid x_t) = \mathcal{N}\!\Big(x_{t-1};\, \sqrt{\alpha_t}\,\mathcal{D}_\theta(x_t, t),\, (1-\alpha_t)\mathbf{I}\Big)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=One%20similarly%20defines%20the%20reverse,s.t.~%7D%20%5Cbeta)).  
  Here $\mathcal{D}_\theta(x_t,t)$ is the *denoiser network* (often a U-Net) with parameters $\theta$, which takes the noisy sample $x_t$ and time index $t$ and predicts the “clean” signal component. In this formulation, $\mathcal{D}_\theta(x_t,t)$ is trained to estimate the previous sample $x_{t-1}$ scaled by $\sqrt{\alpha_t}$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%5Clabel%20%7Beq%3Adenoiser_introduction%7D%20p_%7B%5Ctheta%20%7D%28x_%7Bt,s.t.~%7D%20%5Cbeta)). (Many implementations instead predict the added noise $\epsilon$, but this paper opts for the **$x_0$-prediction formulation** – see below.) The variance $(1-\alpha_t)I$ in (3) is fixed to the same as the forward process variance at step $t$, for simplicity ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%5Clabel%20%7Beq%3Adenoiser_introduction%7D%20p_%7B%5Ctheta%20%7D%28x_%7Bt,s.t.~%7D%20%5Cbeta)).

- **Noise schedule:** as noted, $\alpha_t = 1-\beta_t$ with $\beta_t$ increasing (e.g. linearly) ensures the noise level grows over time ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=where%2C%20the%20D%CE%B8%20is%20the,with%20T%20%3D%201000%20steps)). The authors use a linear schedule and set $T=1000$ steps for diffusion ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=where%2C%20the%20D%CE%B8%20is%20the,However%2C%20for%20training%20the%20model)).

- **“$x_0$-formulation” vs predicting noise:** It’s common in DDPM to have $D_\theta$ predict $\epsilon$ (the noise) instead of $x_{t-1}$, which corresponds to optimizing a *score matching* objective for $q(x_t)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=It%20is%20common%20to%20use,choice%20will%20become%20apparent%20later)). However, HoloDiffusion **employs the $x_0$-prediction formulation**, meaning $D_\theta$ is trained to predict the original clean sample $x_0$ from $x_t$ directly (this relates to Eq. (3) form) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=It%20is%20common%20to%20use,choice%20will%20become%20apparent%20later)). This choice is made because it will facilitate their photometric loss later – the reasoning “will become apparent later” in context of their 3D method ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=in%20eq,comprises%20minimizing%20the%20following%20loss)).

- **Training objective:** Given the forward process, training the diffusion model involves minimizing a **denoising loss**. Using the $x_0$-formulation, the loss is formulated as:  
  ***(Equation 6)*** $\displaystyle \mathcal{L} = \|\,\mathcal{D}_{\theta}(x_t, t) - x_0\,\|^2$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Training.%20Training%20the%20%E2%80%9Cx0,predict%20the%20clean%20sample%20x0)).  
  In words, at a random diffusion time $t$, we take a sample $x_0 \sim p(x)$, generate a noisy $x_t \sim q(x_t \mid x_0)$, and train the network to output the original $x_0$. This is a simple mean squared error encouraging $D_\theta$ to *denoise* $x_t$ back to the clean image $x_0$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Training.%20Training%20the%20%E2%80%9Cx0,predict%20the%20clean%20sample%20x0)). (Equivalently, it could be written as predicting $\epsilon$ and matching that; the two are related by a term $\sqrt{\bar\alpha_t}$, but the authors stick to predicting $x_0$ directly.) Equation (6) is the standard DDPM loss.

- **Sampling procedure:** Once trained, generation (sampling from $p_\theta(x)$) starts with pure Gaussian noise $x_T \sim \mathcal{N}(0,I)$ and iteratively applies the reverse model $T$ times to obtain $x_{T-1}, x_{T-2}, \dots, x_0$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Sampling,x)). Each reverse step draws a sample from the Gaussian in (3). In practice, using the $x_0$ formulation, one can compute the mean for $x_{t-1}$ as $\sqrt{\alpha_t}\mathcal{D}_\theta(x_t,t)$, and sample with the appropriate variance. The paper gives the final sampling formula:  
  ***(Equation 7)*** $x_{t-1} \sim \mathcal{N}\!\Big(\sqrt{\bar\alpha_{\,t-1}}\;\mathcal{D}_\theta(x_t,t),\; (1-\bar\alpha_{\,t-1})\mathbf{I}\Big)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=data%20distribution%20x0%20%E2%88%BC%20q,3D%20Categories%20by%20Watching%20Videos)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=data%20distribution%20x0%20%E2%88%BC%20q,Each%20video%20s)).  
  Here $\bar\alpha_{t-1} = \prod_{i=0}^{t-1} \alpha_i$, and note that as $t\to 0$, $\bar\alpha_{0}\approx 1$ so the mean approaches $\mathcal{D}_\theta(x_1,1)$ with little variance, yielding a sample $x_0$. Equation (7) is essentially the implementation of one reverse diffusion step using the current denoiser output. By repeatedly applying (7) from $t=T$ down to $1$, one obtains a generated data sample $x_0$ distributed as $p_\theta(x)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=data%20distribution%20x0%20%E2%88%BC%20q,3D%20Categories%20by%20Watching%20Videos)). This completes the diffusion model loop.

In summary, Section 3.1 establishes the **notation and standard equations** of diffusion models that HoloDiffusion will build upon. Importantly, the authors emphasize they use the $x_0$-prediction variant (predicting the clean sample) for the denoising network ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=It%20is%20common%20to%20use,choice%20will%20become%20apparent%20later)). This detail is crucial when adapting the objective to 3D, as we will see in Sec. 3.3.

### 3.2. Learning 3D Categories by Watching Videos  
Having covered diffusion basics, the paper now sets up the problem of learning a 3D generative model from 2D videos. The **training data** consists of $N$ video sequences $\{s_i\}_{i=1}^N$, all from the same object category (for example, all videos might be of different car instances) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3,Ii%20j%20%2C%20Pi)). Each video $s_i$ provides a set of frames with their poses:  
$$
s_i = \{(I^i_j,\; P^i_j)\}_{j=1}^{N_{\text{frames}}}, 
$$ 
where $I^i_j \in \mathbb{R}^{3\times H \times W}$ is an RGB image, and $P^i_j \in \mathbb{R}^{4\times 4}$ is the 4×4 camera extrinsic matrix for that frame ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=i%3D1%2C%20each%20de%02picting%20an%20instance,I)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=image%20I%20i%20j%20%E2%88%88,i%20j%20%E2%88%88%20R%204%C3%974)). All images in a video depict the *same object* from different viewpoints (and $P^i_j$ gives the camera position for each view). These camera poses are assumed known (e.g. obtained via structure-from-motion on the video) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=unconditional%203D%20diffusion%20model%20that,rendered%20images%20are%20consistent%20across)).

The goal is to train a generative model of 3D objects *without any 3D ground truth*. In particular, they aim to model a distribution $p(V)$ over some 3D representation $V$ of an object’s shape and appearance ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%2C%20represented%20as%20a%204,voxel%20grids%20V%20%E2%88%88%20R)). HoloDiffusion chooses **3D feature voxel grids** as the representation. Specifically, $V$ is defined as a voxel grid of size $S \times S \times S$, where each voxel contains a $d_V$-dimensional feature vector ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=i%3D1,%E2%88%88%20s%20of%20the%20video)). So $V \in \mathbb{R}^{d_V \times S \times S \times S}$ is essentially a dense latent feature volume. This feature grid is a *learned implicit representation* — it does not directly hold colors or densities but abstract features that will be decoded by a neural renderer (as described later in Sec. 3.4) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=feature%20voxel%20grids%20V%20%E2%88%88,%3A%20R)). Because the features live in a spatial 3D grid, rendering $V$ from any viewpoint will produce a consistent image of some object. The *differentiable rendering function* is denoted $r_\zeta(V, P)$, which takes a feature grid $V$ and a camera pose $P$ and outputs a rendered image $I$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=,%C3%97S%C3%97S%C3%97S%20%C3%97R%204%C3%974%207%E2%86%92%20R)). Formally,  
$$
r_{\zeta}: \;\mathbb{R}^{d_V \times S \times S \times S} \times \mathbb{R}^{4\times4} \;\to\; \mathbb{R}^{3\times H \times W}\,,
$$ 
parametrized by some learned parameters $\zeta$ (this $r_\zeta$ will be implemented via volumetric ray-marching and an MLP, see Sec. 3.4) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=,%C3%97S%C3%97S%C3%97S%20%C3%97R%204%C3%974%207%E2%86%92%20R)). Using this renderer, any voxel grid $V$ can produce an image from a desired viewpoint: $I_j = r_\zeta(V, P_j)$ should represent how the 3D object $V$ looks from camera pose $P_j$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=,d%20V%20%C3%97S%C3%97S%C3%97S%20%C3%97R%204%C3%974)).

The *central challenge* described here is that we cannot directly apply the diffusion framework of Sec. 3.1 to learn $p(V)$, because we do **not have ground-truth samples of $V$** to train on ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20function%20parameters%20%28see%20Sec,problem%20in%20the%20next%20section)). We have plenty of images $I$, but no direct examples of the corresponding feature grids $V$ for each object (those are latent and unknown). Typically, diffusion training (Equation 6) required pairs of clean data samples and their noisy versions to train the denoiser. Here, a “clean sample” would be a 3D feature grid from the true distribution $p(V)$ – but we only have the images that $V$ would produce. In other words, **the distribution $p(V)$ is unobserved**, and we need to infer it indirectly through the images. Simply setting $x = V$ in the diffusion process and trying to optimize the loss would fail because we can’t sample real $V$ grids as targets ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20function%20parameters%20%28see%20Sec,problem%20in%20the%20next%20section)).

The paper thus notes: one might naïvely treat the feature grid as the “data” for diffusion, but *“this does not work because we have no access to ground-truth feature grids $V$ for training”* ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Next%2C%20we%20discuss%20how%20to,problem%20in%20the%20next%20section)). Therefore, the authors devise an approach in the next section to learn the diffusion model **without direct $V$ samples**, by leveraging the available videos. In summary, Section 3.2 sets up that HoloDiffusion needs to learn to generate a voxel feature grid such that its renderings match real images – a task requiring an indirect, bootstrap strategy since the actual 3D latent data is never explicitly given.

### 3.3. Bootstrapped Latent Diffusion Model  
This section introduces the core solution, termed **BLDM (Bootstrapped Latent Diffusion Model)**, which allows learning the 3D denoiser $D_\theta$ using only the videos. The key idea is to use an **auxiliary distribution** of voxel grids, denoted $\bar{V}$, derived from the input images, to train the model, and then refine the model via a bootstrapping procedure so that it generalizes to the true distribution $p(V)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3,%E2%88%9A)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=To%20solve%20this%20issue%2C%20we,2%2C%20our%20idea%20is)).

- **Auxiliary samples $\bar{V}$:** The authors create a proxy for a ground-truth feature grid by **aggregating information from the multiple images of a video**. As illustrated in Fig. 2 (Method Overview) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%202,truth.%20Once)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=The%20auxiliary%20samples%20V%C2%AF%20,grid%20V%C2%AF%20%E2%88%88%20R%20d)), given a training video $s = \{(I_j, P_j)\}$ of an object, they construct an **initial feature grid $\bar{V}$** for that object using a learned function. This process is inspired by the Warp-Conditioned-Embedding (WCE) method [17] ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=to%20obtain%20the%20auxiliary%20samples,%C3%97S%C3%97S%C3%97S%20of%20auxiliary%20features%20V%C2%AF)). In essence, for each voxel coordinate $(m,n,o)$ in the grid (usually normalized to [-1,1]^3 space), they project that 3D point into each image frame of the video using the known camera $P_j$. They then *sample the 2D feature maps* (extracted by a pretrained CNN) at those projected points, and aggregate the features (for example by averaging or using an attention mechanism) from all views to obtain a single $d_V$-dimensional descriptor for that voxel ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=by%20projecting%20the%203D%20coordinate,The%20standard)). By doing this for every voxel, they fill the grid with features. The paper states they use a **frozen ResNet-32 encoder** $E$ to compute 2D image features beforehand ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=corre%02sponding%202D%20image%20features%2C%20and,Instead%2C%20we)). (Detailed specifics of this fusion are in the supplement, but intuitively, this gives a crude reconstruction of the object in feature space, since consistent regions in space will have similar image features from multiple views.) The output is an **auxiliary feature grid $\bar{V} \in \mathbb{R}^{d_V\times S\times S\times S}$** that roughly represents the object from the video ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=design%20strongly%20inspired%20by%20Warp,%E2%88%921%2C%201%5DdV%2018426)). This $\bar{V}$ is *not perfect* – it’s like an initial guess of the 3D object’s features, which may be noisy or incomplete – but it provides a starting point for training. Importantly, one can render $\bar{V}$ with $r_\zeta$ to produce the input images (ideally) if $\bar{V}$ were accurate. Figure 2 depicts this idea of “bootstrapping” a feature grid from video frames.

- **Training with auxiliary diffusion objective:** Now, given $\bar{V}$ for each training object (video), we still cannot directly plug $\bar{V}$ into the diffusion training loss (Equation 6) because $\bar{V}$ is a *deterministic function of the input images*, not a draw from the generative model $p(V)$. Instead, the authors use $\bar{V}$ to simulate noisy training pairs. They introduce a modified diffusion loss called the **photometric loss**. Specifically, they replace the ideal diffusion objective (6) with a loss that compares a *rendered image* of the denoised grid to an actual image:  
  ***(Equation 8)*** $\displaystyle \mathcal{L}_{\text{photo}} := \big\|\, r_{\zeta}\!\big(D_{\theta}(\bar{V}_t,\, t),\; P_j\big) - I_j \big\|^2$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=objective%20which%20does%20not%20require,be%20com%02puted%20because%20the%20image)).  
  Here $\bar{V}_t$ denotes a **noisy version of the auxiliary grid** (analogous to $x_t$ in diffusion, we sample $\bar{V}_t \sim q(\bar{V}_t \mid \bar{V})$ by adding Gaussian noise to $\bar{V}$ features for $t$ time steps). $D_\theta(\bar{V}_t, t)$ is the denoiser network’s output (the predicted clean grid) at time $t$. Then $r_{\zeta}(D_{\theta}(\bar{V}_t,t), P_j)$ renders this denoised grid from the same viewpoint $P_j$ as one of the original images $I_j$. The loss (8) is the mean squared error between this rendered image and the ground-truth image $I_j$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%5Clabel%20,the%20auxiliary%20sample%20V%C2%AF)). In effect, we are training $D_\theta$ such that after denoising a noisy grid, that grid produces the correct image. This **photometric error** drives the features in $D_\theta$’s output to align with what’s needed to explain the real image. Equation (8) *can be computed* during training because $I_j$ and $P_j$ are known from the video, and $\bar{V}_t$ is constructed for that video ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=which%20compares%20the%20rendering%20r%CE%B6,given%20in%20the%20previous%20section)). This is called the **auxiliary denoising diffusion objective** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Auxiliary%20denoising%20diffusion%20objective,I_j%20%5C%7C%5E2%2C%20%288)). It is analogous to (6) but in image space: rather than comparing $D_\theta(\bar{V}_t)$ to a true $V$ (unknown), it compares rendered images to true images as a proxy.

  In summary, using (8), the model $D_\theta$ learns to *remove noise from $\bar{V}_t$ such that the resulting grid produces the correct image*. This trains both the denoiser $\theta$ and the renderer $\zeta$ in conjunction. Notably, this leverages the earlier choice of $x_0$-formulation: $D_\theta$ predicts the clean grid that should produce the image, akin to predicting $x_0$ in the 2D case.

- **Train/test distribution gap:** A challenge arises: during training, $D_\theta$ only ever sees noisy versions of **auxiliary grids $\bar{V}$**, which themselves are derived from real images ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Train%2Ftest%20denoising%20discrepancy,samples%20V%20%E2%88%88%20V%20as)). At test time, however, we want to generate entirely new objects by sampling random noise (pure Gaussian) and denoising it iteratively – i.e. $D_\theta$ will encounter noisy *true* grids $V_t$ that were never constructed from images. There is a potential **distribution mismatch**: $\bar{V}$ (from image aggregation) may have a different distribution than a typical sample from $p(V)$, since $\bar{V}$ carries reconstruction noise and biases from the encoder. In other words, $D_\theta$ is being trained on one kind of input but will be asked to perform on another kind (the ideal generative samples) at inference ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Train%2Ftest%20denoising%20discrepancy,samples%20V%20%E2%88%88%20V%20as)). If not addressed, this could prevent the model from generating good samples from scratch (it might rely on patterns of $\bar{V}$).

- **Two-pass diffusion bootstrapping:** To handle the above discrepancy, the authors introduce a *bootstrapping procedure* that refines the model using its own outputs ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=specified%20by%20eq,finetuning%20D%CE%B8%20as%20a%20result)). The idea is: once $D_\theta$ has learned to denoise the auxiliary inputs well (minimizing $\mathcal{L}_{photo}$), its outputs $D_\theta(\bar{V}_t)$ should be close to *realistic* clean feature grids $V$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=to%20those%2C%20finetuning%20D%CE%B8%20as,optimal%20denoiser%20parameters%20%CE%B8%20%E2%8B%86)). In fact, they assume that at optimum, $D_\theta(\bar{V}_t) \sim V$ (i.e. the denoised auxiliary grid lies in the true data distribution) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=once%20Lphoto%20is%20minimized%2C%20the,noise%20and%20the%20noise%20resulting)). This is a reasonable assumption supported by prior works that optimized photometric loss to recover high-quality 3D shapes ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=D%CE%B8%20%E2%8B%86%20,To%20this)). Given this, they propose to **feed the denoiser’s own output back into the diffusion process** for a second round of denoising. Concretely, after obtaining a denoised grid $D_\theta(\bar{V}_t,t)$, they add noise *again* to this output (at a possibly different time $t'$) and then denoise it once more with $D_\theta$. This *second pass* aims to teach $D_\theta$ how to handle inputs that are closer to true $V$ samples, thus “bootstrapping” the model towards the real distribution.

  They define a **bootstrapped photometric loss** to train this second denoising pass:  
  ***(Equation 9)*** $\displaystyle \mathcal{L}'_{\text{photo}} := \big\|\, r_{\zeta}\!\Big(D_{\theta}\!\big(\epsilon_{t'}(D_{\theta}(\bar{V}_t, t)),\; t'\big),\; P_j\Big) - I_j \big\|^2$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%5Clabel%20,grid%20Z%20at%20time%20t)).  
  This looks similar to (8) but with an inner nesting: $D_{\theta}(\bar{V}_t, t)$ is the first denoised result (an estimate of $V_0$); then $\epsilon_{t'}(\cdot)$ denotes adding noise to that result at some diffusion time $t'$ (using the forward process) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=with%20%CF%B5t%20%E2%80%B2%20,grid%20Z%20at%20time%20t)); and finally $D_{\theta}(\,\epsilon_{t'}(D_{\theta}(\bar{V}_t,t)),\, t')$ is the second denoising output. We render that and compare to the image $I_j$. In simpler terms, $\mathcal{L}'_{photo}$ measures if *two sequential denoising steps* starting from the auxiliary grid can still reproduce the correct image ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=,V%C2%AF%20t%2C%20t%29%2C%20t%E2%80%B2)). The function $\epsilon_{t'}(Z) \sim \mathcal{N}(\sqrt{\bar\alpha_{t'}}\,Z,\ (1-\bar\alpha_{t'})I)$ (mentioned below eq.9) is just the procedure to sample a noisy version of a given grid $Z$ at time $t'$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=with%20%CF%B5t%20%E2%80%B2%20,grid%20Z%20at%20time%20t)). Intuitively, $\mathcal{L}'_{photo}$ forces consistency through a *recurrent denoising*: if $D_\theta$ truly produces a clean grid in the first pass, then adding noise and denoising again should still yield that clean grid (and render correctly). If $D_\theta$’s first output wasn’t on the true manifold, the second pass will deviate – and the loss will penalize that ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=,V%C2%AF%20t%2C%20t%29%2C%20t%E2%80%B2)).

  Using this bootstrapping, the denoiser is **exposed to its own mistakes** and learns to correct them. It bridges the gap between training on $\bar{V}$ and the actual generative diffusion process. After applying both losses (8) and (9), the authors expect that $D_\theta$ has effectively learned to denoise *any* noisy grid that lies on or near the true $V$ distribution.

To summarize Section 3.3: They first train the diffusion model on **auxiliary reconstructions** via a photometric loss, then refine it with a **two-pass strategy (bootstrapping)** so that the model becomes capable of denoising purely sampled noise into coherent 3D feature grids (not just improving the initial $\bar{V}$). This Bootstrapped LDM approach is novel and critical for achieving *unconditional 3D generation* from only images. Figure 2 in the paper illustrates this pipeline: images -> **$\bar{V}$ (auxiliary grid)** -> noising and denoising (first pass) -> noising and denoising again (second pass) -> rendered image, enforcing it matches the input ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=end%2C%20we%20define%20the%20bootstrapped,Z%29%20%E2%88%BC%20N)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%E2%80%B2%20,V%C2%AF%20t%2C%20t%29%2C%20t%E2%80%B2)).

### 3.4. Implementation Details  
Section 3.4 provides practical details on training and the rendering function.

**Training regimen:** HoloDiffusion is trained by minimizing the **sum of the photometric loss and bootstrapped photometric loss**: $\mathcal{L}_{photo} + \mathcal{L}'_{photo}$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Training%20details,until%20convergence%20is%20reached)). They use the Adam optimizer with an initial learning rate of $5\times10^{-5}$, and reduce the LR tenfold whenever the total loss plateaus, continuing until convergence ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Training%20details,until%20convergence%20is%20reached)). Each training iteration proceeds as follows (Training details from Sec. 3.4):

- Randomly sample one training video $s$ (object instance) and then randomly sample 10 source views $\{I_i\}_{i=1}^{10}$ from that video ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=In%20each%20training%20iteration%2C%20we,are%20noised%20to%20form%20V%CB%86)). These 10 images (with their cameras) are used to compute the **auxiliary feature grid $\hat{V}$** for that object via the described fusion method (this $\hat{V}$ corresponds to $\bar{V}$ in previous sections) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=views%20,Vt%29%20is%20noised%20and%20denoised)). 

- Sample a random diffusion timestep $t$ and add noise to $\hat{V}$, yielding $\hat{V}_t$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=i,Vt%29%20is%20noised%20and%20denoised)). Apply the denoiser to get $D_\theta(\hat{V}_t, t)$, the first-pass denoised grid ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=form%20the%20grid%20of%20auxiliary,avoid%20two%20rendering%20passes%20in)).

- Optionally, noise and denoise again for the second pass: in practice, they **do not always do both passes each iteration**. To save computation (specifically, to avoid rendering twice every iteration), they randomly choose to optimize either $\mathcal{L}_{photo}$ *or* $\mathcal{L}'_{photo}$ with 50% probability each iteration ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=t%20and%20later%20denoised%20with,The%20pho%02tometric%20losses%20compare)). So, half the time they only do the first pass and apply loss (8); the other half, they perform the second pass and apply loss (9). This stochastic strategy balances the two objectives without doubling the cost per step ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=t%20and%20later%20denoised%20with,The%20pho%02tometric%20losses%20compare)). In either case, when computing the photometric loss, they compare the rendered output to **3 target views** from the video that are *different from the 10 source views* used to build $\hat{V}$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=L%20%E2%80%B2%20photo%20with%2050,different%20from%20the%20source%20views)). This means the model is tested on held-out views of the object during training, ensuring it learns to generalize to novel viewpoints (which is essential for multi-view consistency).

**Rendering function $r_{\zeta}$:** The differentiable renderer $r_{\zeta}(V, P_j)$ is implemented via a volumetric ray-marching procedure, specifically **Emission-Absorption (EA) raymarching** akin to NeRF’s rendering equation ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Rendering%20function%20r%CE%B6%20,pi%29%20NS)). The paper provides the outline: for each pixel $u$ of the output image, a ray $\mathbf{r}_u$ is cast from the camera defined by pose $P_j$ through the volume ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Rendering%20function%20r%CE%B6%20,pi%29%20NS)). They sample $N_S$ points $\{p_i\}_{i=1}^{N_S}$ along this ray in the 3D space (at regular depth intervals $\Delta$) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=i%3D1%20on%20each%20ray%20at,0%2C%201%5D3%20of%20each)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3D%20point,Ti%2B1%20where%20Ti%20%3D%20e)). For each sample point $p_i$, the feature vector $V[p_i]$ is obtained by **trilinear interpolation** of the voxel grid $V$ (since $p_i$ may not land exactly on a voxel center) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=For%20each%20point%20pi%2C%20we,c%20de%02pends%20on%20the%20ray)). Then a small neural network $f_\zeta$ (an MLP) decodes this feature into **density $\sigma_i$** (a scalar between 0 and 1) and **color $c_i$** (an RGB vector) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%2C%20where%20V%20,pixel%20color%20cru%20%3D%20PNS)). The MLP is designed such that $c_i$ can depend on the viewing direction $\mathbf{r}_u$ (this introduces view-dependent effects like specular highlights), whereas $\sigma_i$ depends only on the spatial location (like occupancy) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=f%CE%B6%20%28V%20,The%20weights%20are)). This setup mirrors NeRF’s use of a view-dependent radiance field ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=f%CE%B6%20%28V%20,The%20weights%20are)).

Finally, given the set of sampled colors and densities along the ray, the pixel color is computed by the standard volume rendering formula:  
$$
c_{\mathbf{r}_u} \;=\; \sum_{i=1}^{N_S} w(p_i)\; c_i, 
$$ 
with weights $w(p_i) = T_i - T_{i+1}$ where $T_i = \exp\!\Big(-\sum_{j=1}^{i-1} \sigma_j \Delta\Big)$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=similar%20to%20NeRF%20,e%20%E2%88%92%20Pi%E2%88%921%201%20%CF%83i%E2%88%86)). In other words, $T_i$ is the transmittance (the fraction of light not yet absorbed) up to the $i$-th sample, and $w(p_i)$ is the amount of light absorbed at the $i$-th sample. This is exactly the equation used in NeRF (Emission-Absorption), ensuring that nearer voxels with high density contribute more to the pixel and occlude farther points ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=similar%20to%20NeRF%20,e%20%E2%88%92%20Pi%E2%88%921%201%20%CF%83i%E2%88%86)). By summing the weighted colors, we get the final rendered pixel color $c_{\mathbf{r}_u}$.

This rendering process $r_\zeta$ is differentiable end-to-end. The parameters $\zeta$ include the weights of the MLP $f_\zeta$ (which maps feature to color/density). During training, $\zeta$ is learned alongside $\theta$ by the photometric losses. The text notes that $\sigma$ is bounded [0,1] and $c$ is in [0,1]^3 (likely via an activation function) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=f%CE%B6%20%28V%20,The%20weights%20are)), and that their $f_\zeta$ is similar to NeRF’s setup. With this renderer, the model can generate realistic images from the feature grid and compare to real images for supervision.

By the end of Section 3 (Implementation Details), the full HoloDiffusion model is specified: a diffusion U-Net $D_\theta$ operating on voxel grids, and a rendering module $r_\zeta$ that turns those grids into images. Training alternates between the one-pass photometric loss and two-pass bootstrapped loss to ensure the model can both reconstruct known views and generalize to generating new objects. This concludes the methodology, and the paper then moves on to experimental evaluation.

## 4. Experiments  
The experimental evaluation (Sec. 4) verifies that HoloDiffusion can learn generative 3D models from real videos and compares it against prior methods. They perform both **quantitative evaluation** of image generation quality and **qualitative analysis** of the 3D consistency of results ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=4,followed%20by%20visualizing%20samples%2018427)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Viewpoint%20change%200%E2%88%98%2060%E2%88%98%20120%E2%88%98,assessing%20the%20quality%20of%20generations)).

**Datasets and Baselines:** The authors use the **CO3Dv2 dataset [46]** for experiments ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=for%20assessing%20the%20quality%20of,shelf)). CO3Dv2 is a large collection of real “fly-around” videos of objects – each video captures an object (e.g. a toy, a plant, etc.) with the camera moving in a circle around it, providing views from all sides ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=CO3Dv2%20,and%20instance%20segmentation%20software%2C%20respectively)). It also provides camera pose annotations (obtained via off-the-shelf Structure-from-Motion) and object masks for each frame ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=The%20dataset%20contains%20videos%20of,For%20each)). From this dataset, they select **4 object categories** to evaluate on: *Apple, Hydrant, TeddyBear,* and *Donut* ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=respectively%29,32GB%20GPUs%20for%202%20weeks)). For each category, they train HoloDiffusion on ~500 training videos (specifically the 500 videos with the highest camera quality scores in that category, as defined in CO3D) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=TeddyBear%20and%20Donut%20for%20our,GAN%20gener%02ates%20radiance%20fields)). This amounts to about 50k images per category (500 videos × ~100 frames each) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=TeddyBear%20and%20Donut%20for%20our,GAN%20gener%02ates%20radiance%20fields)). Training is done on 2 to 8 NVIDIA V100 GPUs with 32GB memory for about 2 weeks per category ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=annotations%2C%20in%20order%20to%20ensure,GAN%20gener%02ates%20radiance%20fields)) – indicating the model is heavy but feasible to train.

They compare HoloDiffusion with three prior 3D generative models: **pi-GAN** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=2,MLPs%20and%20is%20trained%20us%0218428)), **EG3D** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=2,MLPs%20and%20is%20trained%20us%0218428)), and **GET3D** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=2,MLPs%20and%20is%20trained%20us%0218428)). All these baselines also only require 2D supervision (images and possibly poses/masks):

- *pi-GAN* (Chan et al. 2021) generates radiance fields (NeRF representations) via a GAN. It uses MLPs to represent the scene and is trained adversarially on images ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=ing%20an%20adversarial%20objective,fg%2Fbg%20masks%20for%20training%3B%20which)). In the same setting as HoloDiffusion, pi-GAN does not require pose labels (it learns an implicit pose distribution) but because it’s GAN-based and doesn’t enforce multi-view consistency strictly, its results can appear inconsistent across views ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%203,appearance%20variations%20across%20view%20changes)). The text notes pi-GAN’s training is “3D-agnostic” in that it doesn’t leverage actual pose information, and as a result “the 3D neural fields produced by pi-GAN essentially mimic a 2D image GAN” in that dataset ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3D,Figure%203%20further%20analyzes)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Thus%2C%20without%20the%203D,mimic%20a%202D%20image%20GAN)).

- *EG3D* (Chan et al. 2022) is a state-of-the-art 3D-aware GAN that uses a tri-plane representation and a neural renderer, combined with StyleGAN-like training. EG3D **does use pose supervision** during training to get oriented renders ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=ing%20an%20adversarial%20objective,fg%2Fbg%20masks%20for%20training%3B%20which)). It requires both images and their camera poses as input (which are available in CO3Dv2) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=ing%20an%20adversarial%20objective,fg%2Fbg%20masks%20for%20training%3B%20which)). EG3D’s representation is a neural radiance field on a tri-plane, and it can produce high-quality images, but it still trains adversarially. In this setup, EG3D should have an advantage of known poses, but as results show, it doesn’t outperform HoloDiffusion in fidelity.

- *GET3D* (Gao et al. 2022) is another GAN-based model that generates textured 3D meshes. It requires pose annotations and additionally foreground/background masks for training ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20training%20procedure.%20GET3D%20,the%20form%20of%20textured%20meshes)) (the authors supply it with masks from CO3Dv2). GET3D works by generating a 3D shape (mesh) and texture, using a differentiable surface extraction (marching tetrahedra). The output is a mesh, which is then rendered. Because of its mesh representation, GET3D’s outputs are in a different form (not just images, but models). However, it is included to compare generative quality.

The baselines were each trained (presumably on the same dataset splits). Figure 4 in the paper shows **visual comparisons** of samples from pi-GAN, EG3D, GET3D, and HoloDiffusion for the four categories ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Apple%20Hydrant%20TeddyBear%20Donut%204285f4,cam%02era%20poses%20as%20input%20to)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%204,Apart%20from%20this)). From the figure, HoloDiffusion’s samples appear more detailed and consistent (as the authors claim).

**Quantitative evaluation:** They measure two common metrics for generative image quality: **Fréchet Inception Distance (FID)** and **Kernel Inception Distance (KID)** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Quantitative%20evaluation,the%20unaligned%20shapes%20of%20CO3Dv2)). These metrics compare the distribution of generated images to that of real images (lower is better) and are computed per category. Table 1 summarizes FID and KID for each method on each category, and also the average (“Mean”). The table also has a column “VP” indicating whether the method inherently produces view-consistent renders (✓ for HoloDiffusion, EG3D, GET3D which are 3D-based; ✗ for pi-GAN which can be inconsistent) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Table%201,column%20%E2%80%9CVP%E2%80%9D%20denotes%20whether%20renders)).

According to the text, **HoloDiffusion achieves better FID/KID scores than EG3D and GET3D on all categories** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Quantitative%20evaluation,consistency%2C%20the%203D%20neural%20fields)). For example, the paper states HoloDiffusion “produces better scores than EG3D and GET3D” ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%28KID%29%20,categories%2C%20we%20note%20that%20the)). The actual numbers (not all given in text) indicate EG3D and GET3D have significantly higher FIDs (worse) – e.g., EG3D’s FID on Apple is ~170, GET3D’s ~179, whereas HoloDiffusion’s FID on Apple is ~94 (dramatically better) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%28KID%29%20,mimic%20a%202D%20image%20GAN)). On some categories, *pi-GAN* achieves slightly better FID/KID than HoloDiffusion ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%28KID%29%20,essentially%20mimic%20a%202D%20image)). Indeed, pi-GAN’s FID for Apple might be ~93 vs HoloDiffusion’s 94 (very close), and pi-GAN leads on 2 of 4 categories in FID according to the table. However, the authors caution that **pi-GAN’s lower FID does not reflect true 3D quality**, because pi-GAN can exploit 2D image fidelity at the cost of 3D consistency ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=GET3D.%20Although%20pi,all%20the%20methods%20under%20comparison)). They explain that pi-GAN’s training procedure does not enforce a correct 3D shape for the object; it may generate different 2D appearances for different camera angles to minimize image-level loss (mode collapse per view). In CO3Dv2, where objects have varied orientations, pi-GAN cannot recover the actual 3D structure of the objects ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=GET3D.%20Although%20pi,all%20the%20methods%20under%20comparison)). Thus, its FID/KID might be good (since each view looks plausible), but the *set* of views is not consistent. In contrast, HoloDiffusion maintains multi-view coherence (which FID doesn’t directly measure). So, the **takeaway is that HoloDiffusion achieves comparable image quality to the best baseline (pi-GAN) while ensuring genuine 3D consistency**, and it clearly outperforms the other baselines on fidelity metrics ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%28KID%29%20,mimic%20a%202D%20image%20GAN)). The table also includes a variant “HoloDiffusion (No bootstrap)”, which presumably means training with only the first photometric loss and not the second-pass. That variant has much worse scores (the text suggests a very high FID ~343 on Apple) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=match%20at%20L640%20Table%201,column%20%E2%80%9CVP%E2%80%9D%20denotes%20whether%20renders)), indicating the importance of the bootstrapping step for good generation.

**Qualitative evaluation:** Figure 4 (Comparisons) illustrates random samples from each method for the four categories ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Qualitative%20evaluation,videos%20of%20the%20generated%20samples)). The authors note that **HoloDiffusion’s samples are the “most appealing, consistent and realistic”** among all methods ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Qualitative%20evaluation,refer%20to%20the%20project%20webpage)). For instance, HoloDiffusion’s generated apples, hydrants, teddy bears, and donuts exhibit coherent shapes and textures that look like plausible new instances of those categories. By contrast, some baseline outputs have defects: EG3D and GET3D might produce blurrier or less consistent results (especially since GET3D outputs meshes, some detail might be lost in rendering), and pi-GAN, while sometimes realistic per view, lacks consistency across views ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=HOLODIFFUSION%20produces%20the%20most%20appealing%2C,appearance%20is%20inconsistent%20with%20the)). 

Figure 3 (View consistency) provides a specific comparison of multi-view consistency: it shows images of a generated object as the viewpoint rotates from 0° to 360° for HoloDiffusion vs pi-GAN ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Viewpoint%20change%200%E2%88%98%2060%E2%88%98%20120%E2%88%98,appearance%20variations%20across%20view%20changes)). The HoloDiffusion row (top) remains consistent — the object’s appearance stays the same except for the rotation, as one would expect from a true 3D object ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=HoloDiffusion%20pi,appearance%20variations%20across%20view%20changes)). In contrast, pi-GAN’s generated object (bottom row) changes appearance noticeably (e.g. color or shape might flicker) as the viewpoint changes ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%203,appearance%20variations%20across%20view%20changes)). The caption notes that pi-GAN’s results “suffer from significant appearance variations across view changes,” whereas HoloDiffusion’s results remain consistent ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%203,appearance%20variations%20across%20view%20changes)). This visually confirms that HoloDiffusion successfully learns a 3D-consistent representation. The paper also mentions that individual views from pi-GAN can look realistic on their own, but they are not all views of the *same* object (the model effectively learns a “view-dependent trick”) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=and%20realistic%20samples%20among%20all,videos%20of%20the%20generated%20samples)). A reader is referred to the project webpage for more examples and videos of generated samples ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=is%20evident%20that%2C%20although%20individual,videos%20of%20the%20generated%20samples)).

Additionally, Figure 5 (Sampling across time) illustrates the **denoising diffusion process** in HoloDiffusion ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=conjunction%20with%20the%20diffusion%20denoiser%2C,We%2018429)). It likely shows a sequence of renders of a single object (hydrant and teddy bear examples) at different noise levels: starting from pure noise (at $t=T$) and gradually becoming clear at $t=0$. The figure demonstrates that the model can start from a noisy feature grid and converge to a recognizable 3D object through the iterative sampling. This provides intuition that HoloDiffusion indeed performs diffusion in the *3D feature space*: early steps look like random fuzz, and later steps reveal shape and color as noise is removed (the time $t$ decreases from 1000 to 0) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=using%20only%20posed,We%2018429)).

In summary, the experiments show that **HoloDiffusion learns generative models on real-world 3D categories** that produce high-quality images on par with or better than baselines, while also ensuring *true 3D consistency* (which baselines like pi-GAN lack). Quantitatively, HoloDiffusion has the best or very close FID/KID scores among methods that enforce view-consistency ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Quantitative%20evaluation,essentially%20mimic%20a%202D%20image)). Qualitatively, its results are the most consistent and plausible across viewpoints ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Qualitative%20evaluation,videos%20of%20the%20generated%20samples)). The ablation of removing the bootstrap phase results in much worse performance, validating the necessity of the two-pass training. Thus, HoloDiffusion is a significant step forward in 3D-aware image synthesis using diffusion models and only 2D supervision.

## 5. Conclusion  
The paper concludes by reiterating the contributions and results of HoloDiffusion. They state that **HoloDiffusion is an unconditional 3D-consistent generative diffusion model trained using only posed-image supervision** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=5,We)). The core innovation is a *learnable rendering module* integrated with the diffusion denoiser, operating in the feature space of a 3D voxel grid ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3D,We%2018429)). This design, along with the hybrid explicit-implicit representation, decouples the otherwise cubic memory cost of 3D from the final image resolution ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=method%20is%20a%20learnable%20rendering,We)) (i.e., the feature grid can be lower-res, and the renderer adds detail, as noted in the introduction). The conclusion emphasizes that their method can be **trained on raw posed image sets, even with few images**, striking a good balance between quality and diversity ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=5,We)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=method%20is%20a%20learnable%20rendering,We%2018429)). This implies HoloDiffusion doesn’t require thousands of views per object; a reasonably small set (like a single video or few images) can suffice, which is promising for data-scarce situations (though the current experiments still used ~100 views per object). 

They acknowledge the current requirement of known camera poses at training time and suggest that an interesting future direction is to jointly learn the viewpoint estimation along with the generative model ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=conjunction%20with%20the%20diffusion%20denoiser%2C,We%2018429)) (so that external pose estimation isn’t needed). They also mention extending the model for conditional generation scenarios, such as using text guidance or enabling editing of the generated 3D representation, as future work. Additionally, combining multiple objects (multi-class or multi-object scenes) is another avenue, which diffusion models are well-suited for due to their mode-covering nature ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=conjunction%20with%20the%20diffusion%20denoiser%2C,We%2018429)). Notably, they point out that unlike GANs, diffusion models are less prone to mode collapse, so they could potentially excel at multi-class 3D generation without collapsing to a few modes ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=conjunction%20with%20the%20diffusion%20denoiser%2C,We%2018429)).

In closing (and in the optional **Societal Impact** section that follows), the authors reflect on the applications of their work. HoloDiffusion contributes to the generative modeling of 3D assets from real data, which can be used for **creating 3D-consistent content like GIFs or videos** (e.g., imagine creating a rotating 3D avatar from a single image category) in largely positive or harmless ways. They also note that since their models operate on *synthetic data domains* (or non-photoreal objects) and require pose info, they are **not directly applicable to generate realistic human 3D models or similar potentially sensitive content**, reducing the risk of misuse. The released models likely cannot be trivially misused to create malicious deepfakes in 3D, etc., as an added reassurance.

Overall, the conclusion affirms that **HoloDiffusion successfully demonstrates 3D generative diffusion from 2D supervision**, opening up new research directions for more complex scenarios and showing the viability of diffusion models in 3D vision.

## Key Equations and Their Roles  
Below is a list of the main mathematical expressions from the paper, along with explanations of their components and significance in the HoloDiffusion model (with references to the section of the paper where they appear):

1. **Forward Diffusion Transition (Eq. 1, Sec. 3.1):**  
   $q(x_t \mid x_{t-1}) = \mathcal{N}\!\Big(x_t; \sqrt{\alpha_t}\,x_{t-1},\, (1-\alpha_t)\mathbf{I}\Big)\,. $  
   *Explanation:* This equation defines how the **forward diffusion process** adds noise to data at each time step ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%28DDPM%29%20,N%7D%280%2C%20I%29.%20%282)). It says that given a clean sample $x_{t-1}$ at step $t-1$, the distribution of the noised sample $x_t$ at step $t$ is Gaussian with mean $\sqrt{\alpha_t}\,x_{t-1}$ and variance $(1-\alpha_t)I$. Here $\alpha_t \in (0,1)$ is a scheduling parameter (often $\alpha_t = 1-\beta_t$ for a small $\beta_t$) that controls how much signal vs. noise to mix in at step $t$. When $\alpha_t$ is close to 1, $x_t$ will be mostly $x_{t-1}$ plus a little noise; when $\alpha_t$ is smaller, more noise is added. Over many steps, this Markov chain gradually turns an initial sample $x_0$ into pure noise $x_T$. In the paper, Eq. (1) establishes the **forward noising model** that HoloDiffusion assumes for its diffusion framework ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%28DDPM%29%20,N%7D%280%2C%20I%29.%20%282)). The same $\{\alpha_t\}$ schedule is later applied to 3D feature grids $V$.

2. **Reparameterization of Forward Process (Eq. 2, Sec. 3.1):**  
   $x_t = \sqrt{\alpha_t}\,x_{t-1} + \sqrt{\,1-\alpha_t\,}\,\epsilon \qquad\text{with } \epsilon \sim \mathcal{N}(0,I)\,. $  
   *Explanation:* This is a single-step instantiation of Eq. (1) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Samples%20can%20be%20easily%20drawn,N%7D%280%2C%20I%29.%20%282)). It provides a way to **sample $x_t$ directly** by adding scaled random noise to $x_{t-1}$. The term $\sqrt{\alpha_t}\,x_{t-1}$ is the retained signal, and $\sqrt{1-\alpha_t}\,\epsilon$ is the newly added noise. This equation is used for simulation and to derive closed-form relations. For example, by recursive substitution, one can derive that $x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon$ (where $\bar\alpha_t=\prod_{i=1}^t \alpha_i$), which the paper gives as Eq. (5) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20target%20distribution%20p%28x%29,20%2C%2032)). In HoloDiffusion, this property is used to sample noisy versions of 3D grids at arbitrary timesteps and to understand the distribution of those noisy samples (e.g., $\bar{V}_t$ in Eq. 8 is drawn using this kind of formula).

3. **Learned Reverse Diffusion Step (Eq. 3, Sec. 3.1):**  
   $p_{\theta}(x_{t-1}\mid x_t) = \mathcal{N}\!\Big(x_{t-1};\, \sqrt{\alpha_t}\,\mathcal{D}_{\theta}(x_t, t),\, (1-\alpha_t)\mathbf{I}\Big)\,. $  
   *Explanation:* This equation defines the **parametric reverse model** that the diffusion network learns ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=One%20similarly%20defines%20the%20reverse,s.t.~%7D%20%5Cbeta)). It is a Gaussian that predicts $x_{t-1}$ given $x_t$. The mean of this Gaussian is set to $\sqrt{\alpha_t}\,D_{\theta}(x_t,t)$, and the variance is $(1-\alpha_t)I$ (usually fixed or learned in diffusion models; here taken as fixed same as forward). $\mathcal{D}_\theta(x_t,t)$ is the denoiser network’s output when given the noisy input $x_t$ and a time encoding $t$. In an ideal setting, we would have $D_{\theta}(x_t,t) \approx x_{t-1}$ (for predicting the previous sample) or some function of it. HoloDiffusion uses the **$x_0$-formulation**, meaning $D_\theta$ is trained to predict the *original sample* $x_0$ rather than $x_{t-1}$ directly ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=It%20is%20common%20to%20use,choice%20will%20become%20apparent%20later)). But Eq. (3) is written in a form consistent with $x_0$-prediction: notice the mean is $\sqrt{\alpha_t} D_\theta(x_t,t)$, which if $D_\theta(x_t,t) \approx x_0$ (the original data), then $\sqrt{\alpha_t}x_0$ is indeed the expected $x_{t-1}$ for small noise steps (from the forward process, $x_{t-1} \approx \sqrt{\alpha_t}^{-1} x_t$ when $x_t$ is only slightly noisier than $x_{t-1}$). Thus, Eq. (3) ties the network output to the reverse diffusion mean. In the context of HoloDiffusion, this is the model that will be trained on 3D data (with images guiding it). $\theta$ encompasses the weights of the U-Net, and this probabilistic formulation is the basis for sampling new grids via Eq. (7).

4. **Noise Schedule Definition (Eq. 4, Sec. 3.1):**  
   $\alpha_t = 1 - \beta_t,\quad \beta_t \in [0,1], \quad \text{s.t.}\ \beta_t > \beta_{t-1}\ \forall\, t \in [0,T]\,. $  
   *Explanation:* This defines $\alpha_t$ in terms of a series $\beta_t$, which are the actual forward noise variance increments at each step ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=where%2C%20the%20D%CE%B8%20is%20the,with%20T%20%3D%201000%20steps)). The condition $\beta_t > \beta_{t-1}$ ensures the noise level is non-decreasing over time (typically increasing). In practice, a common schedule is linear: $\beta_t = \text{constant}$ or increasing linearly from $\beta_1$ to $\beta_T$. The paper mentions they use a linear schedule with $T=1000$ steps ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=where%2C%20the%20D%CE%B8%20is%20the,However%2C%20for%20training%20the%20model)). While not a dynamic equation per se, Eq. (4) is important as it stipulates how $\alpha_t$ (hence the noise variance $1-\alpha_t$) changes with $t$. In HoloDiffusion, the same schedule is presumably applied for noising the voxel grids. The increasing $\beta_t$ guarantee that as $t$ approaches $T$, $1-\alpha_t$ approaches 1 (complete noise). This schedule is a design choice impacting the diffusion process’s behavior.

5. **Closed-form for Noisy Sample (Eq. 5, Sec. 3.1):**  
   $x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{\,1 - \bar{\alpha}_t\,}\,\epsilon, \qquad \text{where }\bar{\alpha}_t = \prod_{i=0}^{t} \alpha_i\,.$  
   *Explanation:* This equation gives the **direct formula for sampling $x_t$ from the original data $x_0$ in one step** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20target%20distribution%20p%28x%29,20%2C%2032)). It comes from iteratively applying Eq. (2) or known results from DDPM. $\bar{\alpha}_t$ is the cumulative product of $\alpha$’s up to $t$. For example, $\bar{\alpha}_T$ might be a very small number, meaning $x_T \approx \sqrt{1-\bar{\alpha}_T}\,\epsilon \approx \epsilon$ (pure noise). Eq. (5) is useful for training because it allows one to generate $(x_t, x_0)$ pairs without simulating all intermediate steps: you can sample a random $t$ and a random noise $\epsilon$, then combine them with the real $x_0$ to get a valid $x_t$. In the paper, this is exactly how they implement training: *“for training the model we can draw samples $x_t$ directly from $q(x_t|x_0)$”* as given by Eq. (5) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=We%20use%20a%20linear%20time,the%20interpretation%20of%20modeling%20the)). HoloDiffusion will use the same trick for voxel grids: given a current guess of a clean grid $V_0$, they add noise according to $\bar{\alpha}_t$ to get $V_t$ for the loss computations (both in the single-pass and double-pass cases).

6. **Denoising Training Loss (Eq. 6, Sec. 3.1):**  
   $\displaystyle \mathcal{L} = \big\|\, \mathcal{D}_{\theta}(x_t, t) - x_0 \,\big\|^2\,.$  
   *Explanation:* This is the **mean squared error loss used to train the diffusion model** (in the $x_0$-prediction formulation) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Training.%20Training%20the%20%E2%80%9Cx0,predict%20the%20clean%20sample%20x0)). It directly measures the difference between the network’s output $D_\theta(x_t,t)$ and the ground-truth original sample $x_0$. By minimizing this, the network learns to reconstruct $x_0$ from a noisy $x_t$. In conventional diffusion (DDPM), this loss can be derived from the variational lower-bound or simplified as a weighted MSE for noise prediction. The authors adopt this simpler loss. In HoloDiffusion’s context, if $x$ were an image, this would be the image MSE. But for their 3D case, this exact form cannot be applied since $x_0 = V$ (unknown). That is why in Sec. 3.3 they say *“the standard diffusion loss Eq. (6) is unavailable in our case because the data samples $V$ are unavailable”* ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Auxiliary%20denoising%20diffusion%20objective,V%C2%AF)). Instead, they replace Eq. (6) with the photometric loss Eq. (8). Nonetheless, Eq. (6) is the conceptual baseline: it’s what they would use if they had actual 3D grids as training targets. It represents the objective of *denoising diffusion model training*.

7. **Sampling Step (Reverse Diffusion Sampling, Eq. 7, Sec. 3.1):**  
   $x_{t-1} \sim \mathcal{N}\!\Big(\sqrt{\bar{\alpha}_{\,t-1}}\; \mathcal{D}_{\theta}(x_t, t),\;\; (1 - \bar{\alpha}_{\,t-1})\mathbf{I}\Big)\,. $  
   *Explanation:* This equation provides how to **sample the previous latent $x_{t-1}$ given the current $x_t$ during generation** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=data%20distribution%20x0%20%E2%88%BC%20q,3D%20Categories%20by%20Watching%20Videos)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=data%20distribution%20x0%20%E2%88%BC%20q,Each%20video%20s)). It’s essentially combining Eq. (3) and the cumulative product definition: since $\sqrt{\alpha_t}D_\theta(x_t,t)$ is an estimate of $\sqrt{\alpha_t}x_0$ (in $x_0$-formulation) and $\bar{\alpha}_{t-1} = \bar{\alpha}_t / \alpha_t$, one can derive this form. Another way to see it: they choose to reparameterize the reverse process in terms of predicting $x_0$. Many implementations sample as $x_{t-1} = \frac{\sqrt{\alpha_t}(x_t - \sqrt{1-\alpha_t}\,\epsilon_\theta(x_t,t))}{\sqrt{1-\alpha_t}} + $ noise, etc. But here the provided equation is the distribution of $x_{t-1}$. In practice, one would take mean $\mu = \sqrt{\bar{\alpha}_{t-1}}\, D_\theta(x_t,t)$ and sample with variance $(1-\bar{\alpha}_{t-1})I$. This is repeated iteratively from $t=T$ to $1$. Eq. (7) is important as it underpins how HoloDiffusion generates a new 3D feature grid: start from random $\bar{V}_T \sim N(0,I)$ and apply this equation (with $D_\theta$ learned on 3D data) to eventually produce $\bar{V}_0 \sim p(V)$. The paper explicitly references Eq. (7) when discussing the need for bootstrapping: at test, we need $D_\theta$ to handle $V_t$ drawn from $p(V)$ as in Eq. (7), but training only saw $V_t$ from $\bar{V}$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=it%20prevents%20us%20from%20drawing,testing%20sample%20dis%02tributions%20for%20the)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=specified%20by%20eq,finetuning%20D%CE%B8%20as%20a%20result)).

8. **Photometric Loss (Eq. 8, Sec. 3.3):**  
   $\displaystyle \mathcal{L}_{\text{photo}} := \Big\|\, r_\zeta\!\big( \mathcal{D}_\theta (\bar{V}_t,\, t),\; P_j \big) \;-\; I_j \Big\|^2\,.$  
   *Explanation:* This is the **modified diffusion loss for HoloDiffusion’s training**, replacing the inaccessible Eq. (6) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=objective%20which%20does%20not%20require,be%20com%02puted%20because%20the%20image)). Here $\bar{V}_t$ is a noisy auxiliary grid (constructed from training video as described, then noised to level $t$), and $D_\theta(\bar{V}_t,t)$ is the denoised grid predicted by the model. $r_\zeta(\cdot, P_j)$ renders a grid to an image given camera $P_j$. $I_j$ is the actual target image from the video (with pose $P_j$). Thus, $\mathcal{L}_{photo}$ measures the pixel-wise difference between the rendered image of the denoised grid and the real image ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%5Clabel%20,the%20auxiliary%20sample%20V%C2%AF)). All components here come from the training data except $D_\theta$ which is being optimized. $\zeta$ (renderer parameters) is also optimized through this loss, since rendering is differentiable. The role of Eq. (8) is to **train the network to produce correct 3D features such that when rendered they match real images**. Each term inside the norm can be seen as: $r_\zeta(D_\theta(\bar{V}_t,t), P_j)$ is the model’s *reconstruction* of image $I_j$ given the noisy input $\bar{V}_t$. By minimizing (8), if $D_\theta$ perfectly denoises $\bar{V}_t$ to the true underlying grid $V$, then $r_\zeta(V, P_j)$ would equal $I_j$ (assuming the renderer can perfectly reproduce the view). In practice, this loss trains both the denoiser and the feature encoder (via $\bar{V}$) indirectly. Eq. (8) is crucial: it allows learning without ever directly supervising $D_\theta$ on a ground-truth $V$ – the supervision is all through image-domain comparisons ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=which%20compares%20the%20rendering%20r%CE%B6,given%20in%20the%20previous%20section)).

9. **Bootstrapped Photometric Loss (Eq. 9, Sec. 3.3):**  
   $\displaystyle \mathcal{L}'_{\text{photo}} := \Big\|\, r_\zeta\!\Big( \mathcal{D}_\theta\!\big(\epsilon_{t'}( \mathcal{D}_\theta(\bar{V}_t, t) ),\; t' \big),\; P_j \Big) - I_j \Big\|^2\,. $  
   *Explanation:* This equation extends the photometric loss to the **two-pass scenario** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%5Clabel%20,grid%20Z%20at%20time%20t)). Inside it, $D_\theta(\bar{V}_t,t)$ is first-pass denoising of the noisy grid (as before). Then $\epsilon_{t'}(\cdot)$ denotes applying the forward diffusion (adding noise) to that result up to time $t'$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=with%20%CF%B5t%20%E2%80%B2%20,grid%20Z%20at%20time%20t)). In other words, $\epsilon_{t'}(D_\theta(\bar{V}_t,t))$ produces a second-level noisy grid, now starting from the denoiser’s output rather than the auxiliary $\bar{V}$. Next, $D_\theta( \epsilon_{t'}(D_\theta(\bar{V}_t,t)),\, t')$ is the second-pass denoised grid. Finally, $r_\zeta$ renders this doubly-denoised grid from the same view $P_j$, and we compare to the image $I_j$. Essentially, $\mathcal{L}'_{photo}$ checks if **denoising twice in a row still leads to a correct reconstruction** ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=,V%C2%AF%20t%2C%20t%29%2C%20t%E2%80%B2)). The additional notation $\epsilon_{t'}(Z) \sim \mathcal{N}(\sqrt{\bar\alpha_{t'}}Z, (1-\bar\alpha_{t'})I)$ indicates how the noise is applied at time $t'$ ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=with%20%CF%B5t%20%E2%80%B2%20,grid%20Z%20at%20time%20t)), consistent with Eq. (5). The role of Eq. (9) is to **fine-tune the model such that its output after one denoising step is actually a valid sample that it can denoise again**. If $D_\theta(\bar{V}_t,t)$ were already on the true manifold $V$, then adding noise and denoising should return it to the same manifold and yield the same image. If there was any bias or error in $D_\theta$’s first output, the second pass might drift, and the loss (9) will catch that. By minimizing (9), the model learns to correct those subtle errors, improving its ability to handle inputs from its own output distribution ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20discrepancy%20between%20the%20training,the%20clean%20data%20distribution%20V)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=%E2%8B%86%20that%20minimize%20Lphoto,photometric%20loss%20via%20differentiable%20rendering)). In summary, Eq. (9) implements the *bootstrapping* consistency check that aligns the training distribution with the test (sampling) distribution of the denoiser.

Each of these equations plays a distinct part in the overall HoloDiffusion framework. Eqs. (1)–(7) come from the diffusion model theory (Sec. 3.1) and set up how the model thinks about adding and removing noise in general. Eqs. (8) and (9) are novel to this paper, defining how the diffusion model is trained on images via rendering. Together, they ensure that the learned denoiser $D_\theta$ and renderer $r_\zeta$ can generate 3D feature grids whose rendered images match the distribution of real images, thereby achieving the goal of *3D generation from 2D supervision*. The careful design of the loss functions (8) and (9) is what enables HoloDiffusion to succeed where a naive application of Eq. (6) was not possible.

## Final Conclusion Summary  
**HoloDiffusion** represents a breakthrough in training 3D generative models with diffusion techniques using only 2D image data. By introducing a hybrid 3D feature grid representation and a learnable volumetric renderer, the method **decouples the heavy 3D computation from high-resolution image synthesis**, making 3D diffusion training feasible (the renderer adds detail without requiring an extremely high-resolution voxel grid) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=that%20can%20be%20trained%2C%20end,ap%02proaches%20for%203D%20generative%20modeling)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=3D,We%2018429)). The paper’s core contribution is the **Bootstrapped Latent Diffusion Model (BLDM)**, which cleverly sidesteps the need for ground-truth 3D training samples. It does so by first **fusing multi-view images into an approximate 3D feature grid** and training the diffusion denoiser on this proxy via an image-space loss, and then **bootstrapping** the model’s output to refine it further ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Auxiliary%20denoising%20diffusion%20objective,V%C2%AF)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=the%20discrepancy%20between%20the%20training,the%20clean%20data%20distribution%20V)). This two-stage optimization (photometric loss and bootstrapped loss) enables the model to generalize to sampling completely novel 3D structures, not just reconstructing seen ones.

The experimental results confirm that HoloDiffusion can learn complex 3D object distributions (like cars, fruits, toys) from real videos and **generate new 3D-consistent images** with high fidelity. It outperforms or matches state-of-the-art baselines in image quality (FID/KID scores) while providing superior multi-view consistency ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Quantitative%20evaluation,essentially%20mimic%20a%202D%20image)) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%203,appearance%20variations%20across%20view%20changes)). Qualitatively, HoloDiffusion’s samples maintain the same appearance across different viewpoints (a hallmark of true 3D understanding), unlike a baseline like pi-GAN which can output inconsistent views ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=Figure%203,appearance%20variations%20across%20view%20changes)). The model is also shown to scale to real, complex datasets (CO3Dv2) and different object categories, demonstrating robustness.

In conclusion, HoloDiffusion is a **significant step forward in 3D generative modeling**. It merges ideas from neural rendering, multi-view reconstruction, and diffusion probabilistic models into a cohesive framework that learns a 3D generator without 3D supervision. The approach opens up new possibilities: future work can explore integrating automatic camera pose estimation into the training (to remove the need for provided poses) ([HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf#:~:text=conjunction%20with%20the%20diffusion%20denoiser%2C,We%2018429)), conditioning the 3D generation on user inputs like text or sketches, and scaling to multi-object scenes or broader categories. The successful integration of a *diffusion model with a differentiable renderer* in HoloDiffusion paves the way for **creative applications in 3D content creation** – for instance, generating 3D-consistent animations or enabling 3D-aware image editing – using only the wealth of 2D images available. The paper’s results give strong evidence that diffusion models, with appropriate design, can serve as a powerful foundation for generative 3D vision.
